{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "\n",
    "    original_height, original_width, _ = image.shape\n",
    "    target_width, target_height = target_size\n",
    "    resized_image = np.zeros((target_height, target_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(target_height):\n",
    "        for j in range(target_width):\n",
    "            original_x = int(j * original_width / target_width)\n",
    "            original_y = int(i * original_height / target_height)\n",
    "            resized_image[i, j] = image[original_y, original_x]\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "def insertGambar(folder):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    datalabel = [] \n",
    "    path = []\n",
    "    data_distribution = {}\n",
    "    for label in (os.listdir(folder)):\n",
    "        images = os.listdir(folder+label)\n",
    "        data_distribution[label] = len(images)\n",
    "        datalabel.append(label)\n",
    "        print(label)\n",
    "        count = 0\n",
    "        for filename in tqdm(os.listdir(folder+label)):\n",
    "            if count == 100:\n",
    "                break\n",
    "            else :\n",
    "                count += 1\n",
    "            img = cv.imread(os.path.join(folder+label,filename))\n",
    "            if img is not None:\n",
    "                img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "                img = resize_image(img, (150, 150))\n",
    "                filenames.append(filename)\n",
    "                path.append(label)\n",
    "                data.append(img)\n",
    "    return data, filenames, datalabel, path , data_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgerfix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 100/190 [00:01<00:01, 59.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donutfix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 100/160 [00:01<00:01, 51.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friesfix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 100/170 [00:01<00:01, 60.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  300\n",
      "Label:  3\n",
      "Filenames:  300\n",
      "Data Distribution:  {'Burgerfix': 190, 'Donutfix': 160, 'Friesfix': 170}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images, filenames,labels, path, distribusi = insertGambar(\"Train/\") \n",
    "print(\"Data: \", len(images))\n",
    "print(\"Label: \", len(labels))\n",
    "print(\"Filenames: \", len(filenames))\n",
    "print(\"Data Distribution: \", distribusi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_scaling(data):\n",
    "    grayData = []\n",
    "    for img in data:\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        grayData.append(gray)\n",
    "    return grayData\n",
    "\n",
    "gray_img = gray_scaling(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriksTransformasi(matriks):\n",
    "    hasil = np.zeros(matriks.shape)\n",
    "    for i in range(matriks.shape[0]):\n",
    "        for j in range(matriks.shape[1]):\n",
    "            hasil[i][j] = matriks[j][i]\n",
    "    return hasil\n",
    "def sum(matriks):\n",
    "    hasil = 0\n",
    "    for i in range(len(matriks)):\n",
    "        for j in range(len(matriks[0])):\n",
    "            hasil += matriks[i][j]\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derajat(img, derajat):\n",
    "    max = np.max(img)\n",
    "    temp=np.zeros([max+1,max+1])\n",
    "    if derajat == 0:\n",
    "        for i in range (len(img)):\n",
    "            for j in range (len(img[0])-1):\n",
    "                temp[img[i,j],img[i,j+1]] += 1\n",
    "    elif derajat == 45:\n",
    "        for i in range (len (img)-1):\n",
    "            for j in range (len (img[0])-1):\n",
    "                temp[img[i+1,j],img[i,j+1]] += 1\n",
    "    elif derajat == 90:\n",
    "        for i in range (len (img)-1):\n",
    "            for j in range (len (img[0])):\n",
    "                temp[img[i+1,j],img[i,j]] += 1\n",
    "    elif derajat == 135:\n",
    "        for i in range (len (img)-1):\n",
    "            for j in range (len (img[0])-1):\n",
    "                temp[img[i,j],img[i+1,j+1]] += 1\n",
    "    hasil = temp+matriksTransformasi(temp)\n",
    "    total = sum(hasil)\n",
    "    for i in range (len (hasil)):\n",
    "        for j in range (len (hasil)):\n",
    "            hasil[i,j]/=total\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLCM(image):\n",
    "    hasil = []\n",
    "    for img in tqdm(image):\n",
    "        if np.min(img) < 0:\n",
    "            continue\n",
    "        data = []\n",
    "        data.append(derajat(img,0))\n",
    "        data.append(derajat(img,45))\n",
    "        data.append(derajat(img,90))\n",
    "        data.append(derajat(img,135))\n",
    "        hasil.append(data)    \n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:55<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "hasil = GLCM(gray_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast (data):\n",
    "    hasil = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            hasil+=data[i,j]*pow(i-j,2)\n",
    "    return hasil\n",
    "def dissimilarity(data):  \n",
    "    hasil = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            hasil+=data[i,j]*abs(i-j)\n",
    "    return hasil\n",
    "def homogeneity(data):\n",
    "    hasil = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            hasil+=(data[i,j]/(1+pow(i-j,2)))\n",
    "    return hasil\n",
    "def energy(data):\n",
    "    hasil = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            hasil += data[i][j] ** 2\n",
    "    return hasil\n",
    "def correlation(data):\n",
    "    mean = np.zeros((2))\n",
    "    std = np.zeros((2))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            mean[0] += i*data[i,j]\n",
    "            mean[1] += j*data[i,j]\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            std[0] += pow(i-mean[0],2)*data[i,j]\n",
    "            std[1] += pow(j-mean[1],2)*data[i,j]\n",
    "    std[0] = math.sqrt(std[0])\n",
    "    std[1] = math.sqrt(std[1])\n",
    "    hasil = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            hasil += (i-mean[0])*(j-mean[1])*data[i,j]/(std[0]*std[1])\n",
    "    return hasil\n",
    "def entropy(data):\n",
    "    entropy = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            if data[i][j] > 0.0:\n",
    "                entropy += -(data[i][j] * math.log(data[i][j]))\n",
    "    return entropy\n",
    "def asm(data):\n",
    "    asm = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            asm += data[i][j] ** 2\n",
    "    return asm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekstraksi(data):\n",
    "    result = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        extracted_data = []\n",
    "        for j in range(len(data[i])):\n",
    "            contrast_val = contrast(data[i][j])\n",
    "            extracted_data.append(contrast_val)\n",
    "        for j in range(len(data[i])):\n",
    "            dissimilarity_val = dissimilarity(data[i][j])\n",
    "            extracted_data.append(dissimilarity_val)\n",
    "        for j in range(len(data[i])):\n",
    "            homogeneity_val = homogeneity(data[i][j])\n",
    "            extracted_data.append(homogeneity_val)\n",
    "        for j in range(len(data[i])):\n",
    "            energy_val = energy(data[i][j])\n",
    "            extracted_data.append(energy_val)\n",
    "        for j in range(len(data[i])):\n",
    "            correlation_val = correlation(data[i][j])\n",
    "            extracted_data.append(correlation_val)\n",
    "        for j in range(len(data[i])):\n",
    "                asm_val = asm(data[i][j])\n",
    "                extracted_data.append(asm_val)\n",
    "                entropy_val = entropy(data[i][j])\n",
    "                extracted_data.append(entropy_val)\n",
    "        result.append(extracted_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [04:13<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "hasilGLCM = ekstraksi(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(hasilGLCM)):\n",
    "    hasilGLCM[i].append(path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = ['0', '45', '90', '135']\n",
    "fiturs = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "headers = []\n",
    "for fitur in fiturs:\n",
    "    headers.extend([f'{fitur}_0', f'{fitur}_45', f'{fitur}_90', f'{fitur}_135'])\n",
    "for angle in angles:\n",
    "    headers.extend([f'asm_{angle}', f'entropy_{angle}'])\n",
    "headers.append('Path')\n",
    "df = pd.DataFrame(hasilGLCM, columns=headers)\n",
    "df.to_csv('sebelumPreproc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "df = pd.read_csv('sebelumPreproc.csv')\n",
    "x = df.drop('Path', axis=1)\n",
    "y = df['Path']\n",
    "\n",
    "pca = PCA(n_components=25)\n",
    "x = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "xTrain = scaler.fit_transform(xTrain)\n",
    "xTest = scaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5666666666666667\n",
      "Accuracy: 0.7166666666666667\n",
      "Accuracy: 0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric='cosine')\n",
    "knn.fit(xTrain, yTrain)\n",
    "knn_pred = knn.predict(xTest)\n",
    "print('Accuracy:', accuracy_score(yTest, knn_pred))\n",
    "\n",
    "svm = SVC(kernel='rbf', C=30, gamma='auto')\n",
    "svm.fit(xTrain, yTrain)\n",
    "svm_pred = svm.predict(xTest)\n",
    "print('Accuracy:', accuracy_score(yTest, svm_pred))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=500, class_weight='balanced', n_jobs=-1, max_depth=50, min_samples_leaf=1, min_samples_split=2, bootstrap=False, criterion='gini')\n",
    "rfc.fit(xTrain, yTrain)\n",
    "rf_pred = rfc.predict(xTest)\n",
    "print('Accuracy:', accuracy_score(yTest, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1-Score\n",
      "0            KNN  0.566667   0.608632  0.516667  0.479798\n",
      "1            SVM  0.716667   0.713158  0.700000  0.703798\n",
      "2  Random Forest  0.683333   0.734259  0.733333  0.732493\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "knn.fit(xTrain, yTrain)\n",
    "svm.fit(xTrain, yTrain)\n",
    "rf.fit(xTrain, yTrain)\n",
    "\n",
    "yPredKnn = knn.predict(xTest)\n",
    "yPredSvm = svm.predict(xTest)\n",
    "yPredRf = rf.predict(xTest)\n",
    "\n",
    "precision_knn = precision_score(yTest, yPredKnn, average='weighted')\n",
    "recall_knn = recall_score(yTest, yPredKnn, average='weighted')\n",
    "f1_knn = f1_score(yTest, yPredKnn, average='weighted')\n",
    "\n",
    "precision_svm = precision_score(yTest, yPredSvm, average='weighted')\n",
    "recall_svm = recall_score(yTest, yPredSvm, average='weighted')\n",
    "f1_svm = f1_score(yTest, yPredSvm, average='weighted')\n",
    "\n",
    "precision_rf = precision_score(yTest, yPredRf, average='weighted')\n",
    "recall_rf = recall_score(yTest, yPredRf, average='weighted')\n",
    "f1_rf = f1_score(yTest, yPredRf, average='weighted')\n",
    "\n",
    "\n",
    "hasil = pd.DataFrame()\n",
    "hasil['Model'] = ['KNN', 'SVM', 'Random Forest']\n",
    "hasil['Accuracy'] = [accuracy_score(yTest, knn_pred), accuracy_score(yTest, svm_pred), accuracy_score(yTest, rf_pred)]\n",
    "hasil['Precision'] = [precision_knn, precision_svm, precision_rf]\n",
    "hasil['Recall'] = [recall_knn, recall_svm, recall_rf]\n",
    "hasil['F1-Score'] = [f1_knn, f1_svm, f1_rf]\n",
    "print(hasil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
